INFO:root:Reloading encoder from model_1.pth ...
INFO:root:Reloading decoders from model_1.pth ...
DEBUG:root:Encoder: TransformerModel(
  (position_embeddings): Embedding(1024, 1024)
  (lang_embeddings): Embedding(3, 1024)
  (embeddings): Embedding(63961, 1024, padding_idx=2)
  (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  (attentions): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (layer_norm1): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (ffns): ModuleList(
    (0): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (1): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (2): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (3): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (4): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (5): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
  )
  (layer_norm2): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (pred_layer): PredLayer(
    (proj): Linear(in_features=1024, out_features=63961, bias=True)
  )
)
DEBUG:root:Decoder: [TransformerModel(
  (position_embeddings): Embedding(1024, 1024)
  (lang_embeddings): Embedding(3, 1024)
  (embeddings): Embedding(63961, 1024, padding_idx=2)
  (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  (attentions): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (layer_norm1): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (ffns): ModuleList(
    (0): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (1): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (2): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (3): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (4): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (5): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
  )
  (layer_norm2): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (layer_norm15): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (encoder_attn): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (pred_layer): PredLayer(
    (proj): Linear(in_features=1024, out_features=63961, bias=True)
  )
)]
INFO:root:Number of parameters (encoder): 142191065
INFO:root:Number of parameters (decoders): 167393753
INFO:root:Number of decoders: 1
INFO:root:Reloading encoder from model_1.pth ...
INFO:root:Reloading decoders from model_1.pth ...
DEBUG:root:Encoder: TransformerModel(
  (position_embeddings): Embedding(1024, 1024)
  (lang_embeddings): Embedding(3, 1024)
  (embeddings): Embedding(63961, 1024, padding_idx=2)
  (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  (attentions): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (layer_norm1): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (ffns): ModuleList(
    (0): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (1): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (2): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (3): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (4): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (5): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
  )
  (layer_norm2): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (pred_layer): PredLayer(
    (proj): Linear(in_features=1024, out_features=63961, bias=True)
  )
)
DEBUG:root:Decoder: [TransformerModel(
  (position_embeddings): Embedding(1024, 1024)
  (lang_embeddings): Embedding(3, 1024)
  (embeddings): Embedding(63961, 1024, padding_idx=2)
  (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  (attentions): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (layer_norm1): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (ffns): ModuleList(
    (0): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (1): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (2): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (3): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (4): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (5): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
  )
  (layer_norm2): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (layer_norm15): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (encoder_attn): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (pred_layer): PredLayer(
    (proj): Linear(in_features=1024, out_features=63961, bias=True)
  )
)]
INFO:root:Number of parameters (encoder): 142191065
INFO:root:Number of parameters (decoders): 167393753
INFO:root:Number of decoders: 1
INFO:root:Reloading encoder from model_1.pth ...
INFO:root:Reloading decoders from model_1.pth ...
DEBUG:root:Encoder: TransformerModel(
  (position_embeddings): Embedding(1024, 1024)
  (lang_embeddings): Embedding(3, 1024)
  (embeddings): Embedding(63961, 1024, padding_idx=2)
  (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  (attentions): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (layer_norm1): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (ffns): ModuleList(
    (0): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (1): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (2): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (3): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (4): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (5): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
  )
  (layer_norm2): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (pred_layer): PredLayer(
    (proj): Linear(in_features=1024, out_features=63961, bias=True)
  )
)
DEBUG:root:Decoder: [TransformerModel(
  (position_embeddings): Embedding(1024, 1024)
  (lang_embeddings): Embedding(3, 1024)
  (embeddings): Embedding(63961, 1024, padding_idx=2)
  (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  (attentions): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (layer_norm1): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (ffns): ModuleList(
    (0): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (1): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (2): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (3): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (4): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (5): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
  )
  (layer_norm2): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (layer_norm15): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (encoder_attn): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (pred_layer): PredLayer(
    (proj): Linear(in_features=1024, out_features=63961, bias=True)
  )
)]
INFO:root:Number of parameters (encoder): 142191065
INFO:root:Number of parameters (decoders): 167393753
INFO:root:Number of decoders: 1
INFO:root:Reloading encoder from model_1.pth ...
INFO:root:Reloading decoders from model_1.pth ...
DEBUG:root:Encoder: TransformerModel(
  (position_embeddings): Embedding(1024, 1024)
  (lang_embeddings): Embedding(3, 1024)
  (embeddings): Embedding(63961, 1024, padding_idx=2)
  (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  (attentions): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (layer_norm1): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (ffns): ModuleList(
    (0): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (1): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (2): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (3): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (4): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (5): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
  )
  (layer_norm2): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (pred_layer): PredLayer(
    (proj): Linear(in_features=1024, out_features=63961, bias=True)
  )
)
DEBUG:root:Decoder: [TransformerModel(
  (position_embeddings): Embedding(1024, 1024)
  (lang_embeddings): Embedding(3, 1024)
  (embeddings): Embedding(63961, 1024, padding_idx=2)
  (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  (attentions): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (layer_norm1): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (ffns): ModuleList(
    (0): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (1): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (2): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (3): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (4): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (5): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
  )
  (layer_norm2): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (layer_norm15): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (encoder_attn): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (pred_layer): PredLayer(
    (proj): Linear(in_features=1024, out_features=63961, bias=True)
  )
)]
INFO:root:Number of parameters (encoder): 142191065
INFO:root:Number of parameters (decoders): 167393753
INFO:root:Number of decoders: 1
INFO:root:Reloading encoder from model_1.pth ...
INFO:root:Reloading decoders from model_1.pth ...
DEBUG:root:Encoder: TransformerModel(
  (position_embeddings): Embedding(1024, 1024)
  (lang_embeddings): Embedding(3, 1024)
  (embeddings): Embedding(63961, 1024, padding_idx=2)
  (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  (attentions): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (layer_norm1): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (ffns): ModuleList(
    (0): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (1): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (2): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (3): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (4): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (5): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
  )
  (layer_norm2): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (pred_layer): PredLayer(
    (proj): Linear(in_features=1024, out_features=63961, bias=True)
  )
)
DEBUG:root:Decoder: [TransformerModel(
  (position_embeddings): Embedding(1024, 1024)
  (lang_embeddings): Embedding(3, 1024)
  (embeddings): Embedding(63961, 1024, padding_idx=2)
  (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  (attentions): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (layer_norm1): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (ffns): ModuleList(
    (0): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (1): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (2): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (3): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (4): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (5): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
  )
  (layer_norm2): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (layer_norm15): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (encoder_attn): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (pred_layer): PredLayer(
    (proj): Linear(in_features=1024, out_features=63961, bias=True)
  )
)]
INFO:root:Number of parameters (encoder): 142191065
INFO:root:Number of parameters (decoders): 167393753
INFO:root:Number of decoders: 1
INFO:root:Reloading encoder from model_1.pth ...
INFO:root:Reloading decoders from model_1.pth ...
DEBUG:root:Encoder: TransformerModel(
  (position_embeddings): Embedding(1024, 1024)
  (lang_embeddings): Embedding(3, 1024)
  (embeddings): Embedding(63961, 1024, padding_idx=2)
  (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  (attentions): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (layer_norm1): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (ffns): ModuleList(
    (0): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (1): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (2): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (3): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (4): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (5): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
  )
  (layer_norm2): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (pred_layer): PredLayer(
    (proj): Linear(in_features=1024, out_features=63961, bias=True)
  )
)
DEBUG:root:Decoder: [TransformerModel(
  (position_embeddings): Embedding(1024, 1024)
  (lang_embeddings): Embedding(3, 1024)
  (embeddings): Embedding(63961, 1024, padding_idx=2)
  (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  (attentions): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (layer_norm1): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (ffns): ModuleList(
    (0): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (1): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (2): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (3): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (4): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (5): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
  )
  (layer_norm2): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (layer_norm15): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (encoder_attn): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (pred_layer): PredLayer(
    (proj): Linear(in_features=1024, out_features=63961, bias=True)
  )
)]
INFO:root:Number of parameters (encoder): 142191065
INFO:root:Number of parameters (decoders): 167393753
INFO:root:Number of decoders: 1
INFO:root:Reloading encoder from model_1.pth ...
INFO:root:Reloading decoders from model_1.pth ...
DEBUG:root:Encoder: TransformerModel(
  (position_embeddings): Embedding(1024, 1024)
  (lang_embeddings): Embedding(3, 1024)
  (embeddings): Embedding(63961, 1024, padding_idx=2)
  (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  (attentions): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (layer_norm1): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (ffns): ModuleList(
    (0): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (1): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (2): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (3): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (4): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (5): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
  )
  (layer_norm2): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (pred_layer): PredLayer(
    (proj): Linear(in_features=1024, out_features=63961, bias=True)
  )
)
DEBUG:root:Decoder: [TransformerModel(
  (position_embeddings): Embedding(1024, 1024)
  (lang_embeddings): Embedding(3, 1024)
  (embeddings): Embedding(63961, 1024, padding_idx=2)
  (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  (attentions): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (layer_norm1): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (ffns): ModuleList(
    (0): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (1): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (2): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (3): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (4): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (5): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
  )
  (layer_norm2): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (layer_norm15): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (encoder_attn): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (pred_layer): PredLayer(
    (proj): Linear(in_features=1024, out_features=63961, bias=True)
  )
)]
INFO:root:Number of parameters (encoder): 142191065
INFO:root:Number of parameters (decoders): 167393753
INFO:root:Number of decoders: 1
INFO:root:Reloading encoder from model_1.pth ...
INFO:root:Reloading decoders from model_1.pth ...
DEBUG:root:Encoder: TransformerModel(
  (position_embeddings): Embedding(1024, 1024)
  (lang_embeddings): Embedding(3, 1024)
  (embeddings): Embedding(63961, 1024, padding_idx=2)
  (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  (attentions): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (layer_norm1): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (ffns): ModuleList(
    (0): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (1): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (2): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (3): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (4): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (5): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
  )
  (layer_norm2): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (pred_layer): PredLayer(
    (proj): Linear(in_features=1024, out_features=63961, bias=True)
  )
)
DEBUG:root:Decoder: [TransformerModel(
  (position_embeddings): Embedding(1024, 1024)
  (lang_embeddings): Embedding(3, 1024)
  (embeddings): Embedding(63961, 1024, padding_idx=2)
  (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  (attentions): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (layer_norm1): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (ffns): ModuleList(
    (0): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (1): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (2): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (3): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (4): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (5): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
  )
  (layer_norm2): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (layer_norm15): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (encoder_attn): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (pred_layer): PredLayer(
    (proj): Linear(in_features=1024, out_features=63961, bias=True)
  )
)]
INFO:root:Number of parameters (encoder): 142191065
INFO:root:Number of parameters (decoders): 167393753
INFO:root:Number of decoders: 1
INFO:root:Reloading encoder from model_1.pth ...
INFO:root:Reloading decoders from model_1.pth ...
DEBUG:root:Encoder: TransformerModel(
  (position_embeddings): Embedding(1024, 1024)
  (lang_embeddings): Embedding(3, 1024)
  (embeddings): Embedding(63961, 1024, padding_idx=2)
  (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  (attentions): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (layer_norm1): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (ffns): ModuleList(
    (0): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (1): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (2): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (3): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (4): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (5): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
  )
  (layer_norm2): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (pred_layer): PredLayer(
    (proj): Linear(in_features=1024, out_features=63961, bias=True)
  )
)
DEBUG:root:Decoder: [TransformerModel(
  (position_embeddings): Embedding(1024, 1024)
  (lang_embeddings): Embedding(3, 1024)
  (embeddings): Embedding(63961, 1024, padding_idx=2)
  (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  (attentions): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (layer_norm1): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (ffns): ModuleList(
    (0): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (1): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (2): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (3): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (4): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (5): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
  )
  (layer_norm2): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (layer_norm15): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (encoder_attn): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (pred_layer): PredLayer(
    (proj): Linear(in_features=1024, out_features=63961, bias=True)
  )
)]
INFO:root:Number of parameters (encoder): 142191065
INFO:root:Number of parameters (decoders): 167393753
INFO:root:Number of decoders: 1
INFO:root:Reloading encoder from model_1.pth ...
INFO:root:Reloading decoders from model_1.pth ...
DEBUG:root:Encoder: TransformerModel(
  (position_embeddings): Embedding(1024, 1024)
  (lang_embeddings): Embedding(3, 1024)
  (embeddings): Embedding(63961, 1024, padding_idx=2)
  (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  (attentions): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (layer_norm1): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (ffns): ModuleList(
    (0): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (1): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (2): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (3): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (4): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (5): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
  )
  (layer_norm2): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (pred_layer): PredLayer(
    (proj): Linear(in_features=1024, out_features=63961, bias=True)
  )
)
DEBUG:root:Decoder: [TransformerModel(
  (position_embeddings): Embedding(1024, 1024)
  (lang_embeddings): Embedding(3, 1024)
  (embeddings): Embedding(63961, 1024, padding_idx=2)
  (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  (attentions): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (layer_norm1): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (ffns): ModuleList(
    (0): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (1): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (2): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (3): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (4): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (5): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
  )
  (layer_norm2): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (layer_norm15): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (encoder_attn): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (pred_layer): PredLayer(
    (proj): Linear(in_features=1024, out_features=63961, bias=True)
  )
)]
INFO:root:Number of parameters (encoder): 142191065
INFO:root:Number of parameters (decoders): 167393753
INFO:root:Number of decoders: 1
INFO:root:Reloading encoder from model_1.pth ...
INFO:root:Reloading decoders from model_1.pth ...
DEBUG:root:Encoder: TransformerModel(
  (position_embeddings): Embedding(1024, 1024)
  (lang_embeddings): Embedding(3, 1024)
  (embeddings): Embedding(63961, 1024, padding_idx=2)
  (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  (attentions): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (layer_norm1): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (ffns): ModuleList(
    (0): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (1): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (2): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (3): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (4): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (5): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
  )
  (layer_norm2): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (pred_layer): PredLayer(
    (proj): Linear(in_features=1024, out_features=63961, bias=True)
  )
)
DEBUG:root:Decoder: [TransformerModel(
  (position_embeddings): Embedding(1024, 1024)
  (lang_embeddings): Embedding(3, 1024)
  (embeddings): Embedding(63961, 1024, padding_idx=2)
  (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  (attentions): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (layer_norm1): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (ffns): ModuleList(
    (0): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (1): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (2): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (3): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (4): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (5): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
  )
  (layer_norm2): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (layer_norm15): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (encoder_attn): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (pred_layer): PredLayer(
    (proj): Linear(in_features=1024, out_features=63961, bias=True)
  )
)]
INFO:root:Number of parameters (encoder): 142191065
INFO:root:Number of parameters (decoders): 167393753
INFO:root:Number of decoders: 1
INFO:root:Reloading encoder from model_1.pth ...
INFO:root:Reloading decoders from model_1.pth ...
DEBUG:root:Encoder: TransformerModel(
  (position_embeddings): Embedding(1024, 1024)
  (lang_embeddings): Embedding(3, 1024)
  (embeddings): Embedding(63961, 1024, padding_idx=2)
  (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  (attentions): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (layer_norm1): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (ffns): ModuleList(
    (0): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (1): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (2): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (3): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (4): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (5): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
  )
  (layer_norm2): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (pred_layer): PredLayer(
    (proj): Linear(in_features=1024, out_features=63961, bias=True)
  )
)
DEBUG:root:Decoder: [TransformerModel(
  (position_embeddings): Embedding(1024, 1024)
  (lang_embeddings): Embedding(3, 1024)
  (embeddings): Embedding(63961, 1024, padding_idx=2)
  (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  (attentions): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (layer_norm1): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (ffns): ModuleList(
    (0): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (1): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (2): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (3): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (4): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (5): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
  )
  (layer_norm2): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (layer_norm15): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (encoder_attn): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (pred_layer): PredLayer(
    (proj): Linear(in_features=1024, out_features=63961, bias=True)
  )
)]
INFO:root:Number of parameters (encoder): 142191065
INFO:root:Number of parameters (decoders): 167393753
INFO:root:Number of decoders: 1
WARNING:werkzeug: * Running on all addresses.
   WARNING: This is a development server. Do not use it in a production deployment.
INFO:werkzeug: * Running on http://172.17.0.2:5000/ (Press CTRL+C to quit)
INFO:werkzeug: * Restarting with stat
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 582-802-413
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 19:18:16] "GET / HTTP/1.1" 200 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 19:18:16] "GET / HTTP/1.1" 200 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 19:18:16] "GET / HTTP/1.1" 200 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 19:18:16] "GET / HTTP/1.1" 200 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 19:18:16] "GET /static/css/2.1ca50200.chunk.css HTTP/1.1" 200 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 19:18:16] "GET /static/js/main.d4ca551a.chunk.js HTTP/1.1" 200 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 19:18:16] "GET /static/css/main.ea51cd8d.chunk.css HTTP/1.1" 200 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 19:18:16] "GET /static/js/2.0a1eb33d.chunk.js HTTP/1.1" 200 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 19:18:16] "GET /translate.png HTTP/1.1" 200 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 19:18:16] "GET /static/media/roboto-latin-500.f5b74d7f.woff2 HTTP/1.1" 200 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 19:18:16] "GET /static/media/roboto-latin-400italic.d022bc70.woff2 HTTP/1.1" 200 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 19:18:16] "GET /static/media/roboto-latin-700.c18ee39f.woff2 HTTP/1.1" 200 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 19:18:16] "GET /static/media/roboto-latin-400.176f8f5b.woff2 HTTP/1.1" 200 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 19:18:17] "GET /manifest.json HTTP/1.1" 200 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 19:18:17] "GET /translate.png HTTP/1.1" 200 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 19:18:17] "GET /logo192.png HTTP/1.1" 200 -
INFO:root:Reloading encoder from model_1.pth ...
INFO:root:Reloading decoders from model_1.pth ...
DEBUG:root:Encoder: TransformerModel(
  (position_embeddings): Embedding(1024, 1024)
  (lang_embeddings): Embedding(3, 1024)
  (embeddings): Embedding(63961, 1024, padding_idx=2)
  (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  (attentions): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (layer_norm1): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (ffns): ModuleList(
    (0): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (1): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (2): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (3): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (4): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (5): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
  )
  (layer_norm2): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (pred_layer): PredLayer(
    (proj): Linear(in_features=1024, out_features=63961, bias=True)
  )
)
DEBUG:root:Decoder: [TransformerModel(
  (position_embeddings): Embedding(1024, 1024)
  (lang_embeddings): Embedding(3, 1024)
  (embeddings): Embedding(63961, 1024, padding_idx=2)
  (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  (attentions): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (layer_norm1): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (ffns): ModuleList(
    (0): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (1): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (2): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (3): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (4): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (5): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
  )
  (layer_norm2): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (layer_norm15): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (encoder_attn): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (pred_layer): PredLayer(
    (proj): Linear(in_features=1024, out_features=63961, bias=True)
  )
)]
INFO:root:Number of parameters (encoder): 142191065
INFO:root:Number of parameters (decoders): 167393753
INFO:root:Number of decoders: 1
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 19:18:51] "POST /translate HTTP/1.1" 200 -
WARNING:werkzeug: * Running on all addresses.
   WARNING: This is a development server. Do not use it in a production deployment.
INFO:werkzeug: * Running on http://172.17.0.2:5000/ (Press CTRL+C to quit)
INFO:werkzeug: * Restarting with stat
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 633-934-218
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 21:46:56] "GET / HTTP/1.1" 200 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 21:46:56] "GET / HTTP/1.1" 200 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 21:46:56] "GET /static/css/main.ea51cd8d.chunk.css HTTP/1.1" 200 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 21:46:56] "GET /static/css/2.1ca50200.chunk.css HTTP/1.1" 200 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 21:46:56] "GET /static/js/2.0a1eb33d.chunk.js HTTP/1.1" 200 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 21:46:56] "GET /static/js/main.d4ca551a.chunk.js HTTP/1.1" 200 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 21:46:56] "GET /translate.png HTTP/1.1" 200 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 21:46:56] "GET /static/media/roboto-latin-500.f5b74d7f.woff2 HTTP/1.1" 200 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 21:46:56] "GET /static/media/roboto-latin-400.176f8f5b.woff2 HTTP/1.1" 200 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 21:46:56] "GET /static/media/roboto-latin-400italic.d022bc70.woff2 HTTP/1.1" 200 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 21:46:56] "GET /static/media/roboto-latin-700.c18ee39f.woff2 HTTP/1.1" 200 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 21:46:57] "GET /manifest.json HTTP/1.1" 200 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 21:46:57] "GET /logo192.png HTTP/1.1" 200 -
INFO:root:Reloading encoder from model_1.pth ...
INFO:root:Reloading decoders from model_1.pth ...
DEBUG:root:Encoder: TransformerModel(
  (position_embeddings): Embedding(1024, 1024)
  (lang_embeddings): Embedding(3, 1024)
  (embeddings): Embedding(63961, 1024, padding_idx=2)
  (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  (attentions): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (layer_norm1): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (ffns): ModuleList(
    (0): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (1): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (2): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (3): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (4): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (5): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
  )
  (layer_norm2): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (pred_layer): PredLayer(
    (proj): Linear(in_features=1024, out_features=63961, bias=True)
  )
)
DEBUG:root:Decoder: [TransformerModel(
  (position_embeddings): Embedding(1024, 1024)
  (lang_embeddings): Embedding(3, 1024)
  (embeddings): Embedding(63961, 1024, padding_idx=2)
  (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  (attentions): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (layer_norm1): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (ffns): ModuleList(
    (0): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (1): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (2): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (3): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (4): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (5): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
  )
  (layer_norm2): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (layer_norm15): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (encoder_attn): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (pred_layer): PredLayer(
    (proj): Linear(in_features=1024, out_features=63961, bias=True)
  )
)]
INFO:root:Number of parameters (encoder): 142191065
INFO:root:Number of parameters (decoders): 167393753
INFO:root:Number of decoders: 1
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 21:47:54] "POST /translate HTTP/1.1" 200 -
INFO:root:Reloading encoder from model_1.pth ...
INFO:root:Reloading decoders from model_1.pth ...
DEBUG:root:Encoder: TransformerModel(
  (position_embeddings): Embedding(1024, 1024)
  (lang_embeddings): Embedding(3, 1024)
  (embeddings): Embedding(63961, 1024, padding_idx=2)
  (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  (attentions): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (layer_norm1): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (ffns): ModuleList(
    (0): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (1): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (2): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (3): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (4): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (5): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
  )
  (layer_norm2): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (pred_layer): PredLayer(
    (proj): Linear(in_features=1024, out_features=63961, bias=True)
  )
)
DEBUG:root:Decoder: [TransformerModel(
  (position_embeddings): Embedding(1024, 1024)
  (lang_embeddings): Embedding(3, 1024)
  (embeddings): Embedding(63961, 1024, padding_idx=2)
  (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  (attentions): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (layer_norm1): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (ffns): ModuleList(
    (0): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (1): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (2): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (3): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (4): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (5): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
  )
  (layer_norm2): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (layer_norm15): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (encoder_attn): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (pred_layer): PredLayer(
    (proj): Linear(in_features=1024, out_features=63961, bias=True)
  )
)]
INFO:root:Number of parameters (encoder): 142191065
INFO:root:Number of parameters (decoders): 167393753
INFO:root:Number of decoders: 1
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 21:48:19] "POST /translate HTTP/1.1" 200 -
WARNING:werkzeug: * Running on all addresses.
   WARNING: This is a development server. Do not use it in a production deployment.
INFO:werkzeug: * Running on http://172.17.0.2:5000/ (Press CTRL+C to quit)
INFO:werkzeug: * Restarting with stat
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 730-413-581
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 21:52:18] "GET / HTTP/1.1" 200 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 21:52:18] "[36mGET /static/js/2.0a1eb33d.chunk.js HTTP/1.1[0m" 304 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 21:52:18] "[36mGET /static/css/2.1ca50200.chunk.css HTTP/1.1[0m" 304 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 21:52:18] "[36mGET /static/js/main.d4ca551a.chunk.js HTTP/1.1[0m" 304 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 21:52:18] "[36mGET /static/css/main.ea51cd8d.chunk.css HTTP/1.1[0m" 304 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 21:52:18] "[36mGET /translate.png HTTP/1.1[0m" 304 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 21:52:18] "[36mGET /static/media/roboto-latin-500.f5b74d7f.woff2 HTTP/1.1[0m" 304 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 21:52:18] "[36mGET /static/media/roboto-latin-400italic.d022bc70.woff2 HTTP/1.1[0m" 304 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 21:52:18] "[36mGET /static/media/roboto-latin-400.176f8f5b.woff2 HTTP/1.1[0m" 304 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 21:52:18] "[36mGET /static/media/roboto-latin-700.c18ee39f.woff2 HTTP/1.1[0m" 304 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 21:52:18] "[36mGET /manifest.json HTTP/1.1[0m" 304 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 21:52:18] "GET /translate.png HTTP/1.1" 200 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 21:52:18] "[36mGET /logo192.png HTTP/1.1[0m" 304 -
INFO:root:Reloading encoder from model_1.pth ...
INFO:root:Reloading decoders from model_1.pth ...
DEBUG:root:Encoder: TransformerModel(
  (position_embeddings): Embedding(1024, 1024)
  (lang_embeddings): Embedding(3, 1024)
  (embeddings): Embedding(63961, 1024, padding_idx=2)
  (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  (attentions): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (layer_norm1): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (ffns): ModuleList(
    (0): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (1): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (2): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (3): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (4): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (5): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
  )
  (layer_norm2): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (pred_layer): PredLayer(
    (proj): Linear(in_features=1024, out_features=63961, bias=True)
  )
)
DEBUG:root:Decoder: [TransformerModel(
  (position_embeddings): Embedding(1024, 1024)
  (lang_embeddings): Embedding(3, 1024)
  (embeddings): Embedding(63961, 1024, padding_idx=2)
  (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  (attentions): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (layer_norm1): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (ffns): ModuleList(
    (0): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (1): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (2): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (3): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (4): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (5): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
  )
  (layer_norm2): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (layer_norm15): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (encoder_attn): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (pred_layer): PredLayer(
    (proj): Linear(in_features=1024, out_features=63961, bias=True)
  )
)]
INFO:root:Number of parameters (encoder): 142191065
INFO:root:Number of parameters (decoders): 167393753
INFO:root:Number of decoders: 1
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 21:54:24] "POST /translate HTTP/1.1" 200 -
INFO:werkzeug: * Detected change in '/app/server/outputProcessing/outputFiles/inputFile.py', reloading
INFO:werkzeug: * Restarting with stat
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 730-413-581
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 21:56:53] "POST /translate HTTP/1.1" 200 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 21:56:53] "[36mGET /manifest.json HTTP/1.1[0m" 304 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 21:56:53] "[36mGET /static/css/2.1ca50200.chunk.css HTTP/1.1[0m" 304 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 21:56:53] "[36mGET /static/css/main.ea51cd8d.chunk.css HTTP/1.1[0m" 304 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 21:56:53] "GET /static/js/2.0a1eb33d.chunk.js.map HTTP/1.1" 200 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 21:56:53] "GET /static/js/main.d4ca551a.chunk.js.map HTTP/1.1" 200 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 21:56:53] "POST /translate HTTP/1.1" 200 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 21:56:53] "GET /static/css/main.ea51cd8d.chunk.css.map HTTP/1.1" 200 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 21:56:53] "GET /static/css/2.1ca50200.chunk.css.map HTTP/1.1" 200 -
INFO:root:Reloading encoder from model_1.pth ...
INFO:root:Reloading encoder from model_1.pth ...
INFO:root:Reloading decoders from model_1.pth ...
INFO:root:Reloading decoders from model_1.pth ...
DEBUG:root:Encoder: TransformerModel(
  (position_embeddings): Embedding(1024, 1024)
  (lang_embeddings): Embedding(3, 1024)
  (embeddings): Embedding(63961, 1024, padding_idx=2)
  (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  (attentions): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (layer_norm1): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (ffns): ModuleList(
    (0): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (1): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (2): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (3): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (4): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (5): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
  )
  (layer_norm2): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (pred_layer): PredLayer(
    (proj): Linear(in_features=1024, out_features=63961, bias=True)
  )
)
DEBUG:root:Decoder: [TransformerModel(
  (position_embeddings): Embedding(1024, 1024)
  (lang_embeddings): Embedding(3, 1024)
  (embeddings): Embedding(63961, 1024, padding_idx=2)
  (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  (attentions): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (layer_norm1): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (ffns): ModuleList(
    (0): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (1): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (2): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (3): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (4): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (5): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
  )
  (layer_norm2): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (layer_norm15): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (encoder_attn): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (pred_layer): PredLayer(
    (proj): Linear(in_features=1024, out_features=63961, bias=True)
  )
)]
INFO:root:Number of parameters (encoder): 142191065
INFO:root:Number of parameters (decoders): 167393753
INFO:root:Number of decoders: 1
DEBUG:root:Encoder: TransformerModel(
  (position_embeddings): Embedding(1024, 1024)
  (lang_embeddings): Embedding(3, 1024)
  (embeddings): Embedding(63961, 1024, padding_idx=2)
  (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  (attentions): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (layer_norm1): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (ffns): ModuleList(
    (0): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (1): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (2): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (3): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (4): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (5): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
  )
  (layer_norm2): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (pred_layer): PredLayer(
    (proj): Linear(in_features=1024, out_features=63961, bias=True)
  )
)
DEBUG:root:Decoder: [TransformerModel(
  (position_embeddings): Embedding(1024, 1024)
  (lang_embeddings): Embedding(3, 1024)
  (embeddings): Embedding(63961, 1024, padding_idx=2)
  (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  (attentions): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (layer_norm1): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (ffns): ModuleList(
    (0): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (1): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (2): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (3): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (4): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (5): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
  )
  (layer_norm2): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (layer_norm15): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (encoder_attn): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (pred_layer): PredLayer(
    (proj): Linear(in_features=1024, out_features=63961, bias=True)
  )
)]
INFO:root:Number of parameters (encoder): 142191065
INFO:root:Number of parameters (decoders): 167393753
INFO:root:Number of decoders: 1
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 21:57:19] "POST /translate HTTP/1.1" 200 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 21:57:20] "POST /translate HTTP/1.1" 200 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 21:57:34] "POST /translate HTTP/1.1" 200 -
INFO:werkzeug: * Detected change in '/app/translate.py', reloading
INFO:werkzeug: * Restarting with stat
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 730-413-581
WARNING:werkzeug: * Running on all addresses.
   WARNING: This is a development server. Do not use it in a production deployment.
INFO:werkzeug: * Running on http://172.17.0.2:5000/ (Press CTRL+C to quit)
INFO:werkzeug: * Restarting with stat
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 141-049-749
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 22:02:10] "GET / HTTP/1.1" 200 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 22:02:10] "GET / HTTP/1.1" 200 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 22:02:10] "[36mGET /static/js/main.d4ca551a.chunk.js HTTP/1.1[0m" 304 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 22:02:10] "[36mGET /static/css/main.ea51cd8d.chunk.css HTTP/1.1[0m" 304 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 22:02:10] "[36mGET /static/js/2.0a1eb33d.chunk.js HTTP/1.1[0m" 304 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 22:02:10] "[36mGET /static/css/2.1ca50200.chunk.css HTTP/1.1[0m" 304 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 22:02:10] "[36mGET /translate.png HTTP/1.1[0m" 304 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 22:02:10] "[36mGET /static/media/roboto-latin-500.f5b74d7f.woff2 HTTP/1.1[0m" 304 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 22:02:10] "[36mGET /static/media/roboto-latin-400.176f8f5b.woff2 HTTP/1.1[0m" 304 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 22:02:10] "[36mGET /static/media/roboto-latin-400italic.d022bc70.woff2 HTTP/1.1[0m" 304 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 22:02:10] "[36mGET /static/media/roboto-latin-700.c18ee39f.woff2 HTTP/1.1[0m" 304 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 22:02:11] "[36mGET /manifest.json HTTP/1.1[0m" 304 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 22:02:11] "GET /translate.png HTTP/1.1" 200 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 22:02:11] "[36mGET /logo192.png HTTP/1.1[0m" 304 -
INFO:root:Reloading encoder from model_1.pth ...
INFO:root:Reloading decoders from model_1.pth ...
DEBUG:root:Encoder: TransformerModel(
  (position_embeddings): Embedding(1024, 1024)
  (lang_embeddings): Embedding(3, 1024)
  (embeddings): Embedding(63961, 1024, padding_idx=2)
  (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  (attentions): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (layer_norm1): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (ffns): ModuleList(
    (0): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (1): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (2): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (3): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (4): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (5): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
  )
  (layer_norm2): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (pred_layer): PredLayer(
    (proj): Linear(in_features=1024, out_features=63961, bias=True)
  )
)
DEBUG:root:Decoder: [TransformerModel(
  (position_embeddings): Embedding(1024, 1024)
  (lang_embeddings): Embedding(3, 1024)
  (embeddings): Embedding(63961, 1024, padding_idx=2)
  (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  (attentions): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (layer_norm1): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (ffns): ModuleList(
    (0): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (1): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (2): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (3): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (4): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (5): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
  )
  (layer_norm2): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (layer_norm15): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (encoder_attn): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (pred_layer): PredLayer(
    (proj): Linear(in_features=1024, out_features=63961, bias=True)
  )
)]
INFO:root:Number of parameters (encoder): 142191065
INFO:root:Number of parameters (decoders): 167393753
INFO:root:Number of decoders: 1
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 22:02:39] "POST /translate HTTP/1.1" 200 -
INFO:root:Reloading encoder from model_1.pth ...
INFO:root:Reloading decoders from model_1.pth ...
DEBUG:root:Encoder: TransformerModel(
  (position_embeddings): Embedding(1024, 1024)
  (lang_embeddings): Embedding(3, 1024)
  (embeddings): Embedding(63961, 1024, padding_idx=2)
  (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  (attentions): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (layer_norm1): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (ffns): ModuleList(
    (0): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (1): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (2): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (3): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (4): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (5): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
  )
  (layer_norm2): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (pred_layer): PredLayer(
    (proj): Linear(in_features=1024, out_features=63961, bias=True)
  )
)
DEBUG:root:Decoder: [TransformerModel(
  (position_embeddings): Embedding(1024, 1024)
  (lang_embeddings): Embedding(3, 1024)
  (embeddings): Embedding(63961, 1024, padding_idx=2)
  (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  (attentions): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (layer_norm1): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (ffns): ModuleList(
    (0): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (1): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (2): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (3): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (4): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (5): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
  )
  (layer_norm2): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (layer_norm15): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (encoder_attn): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (pred_layer): PredLayer(
    (proj): Linear(in_features=1024, out_features=63961, bias=True)
  )
)]
INFO:root:Number of parameters (encoder): 142191065
INFO:root:Number of parameters (decoders): 167393753
INFO:root:Number of decoders: 1
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 22:03:15] "POST /translate HTTP/1.1" 200 -
WARNING:werkzeug: * Running on all addresses.
   WARNING: This is a development server. Do not use it in a production deployment.
INFO:werkzeug: * Running on http://172.17.0.2:5000/ (Press CTRL+C to quit)
INFO:werkzeug: * Restarting with stat
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 279-760-745
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 22:06:15] "GET / HTTP/1.1" 200 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 22:06:15] "[36mGET /static/css/main.ea51cd8d.chunk.css HTTP/1.1[0m" 304 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 22:06:15] "[36mGET /static/css/2.1ca50200.chunk.css HTTP/1.1[0m" 304 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 22:06:15] "[36mGET /static/js/2.0a1eb33d.chunk.js HTTP/1.1[0m" 304 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 22:06:15] "[36mGET /static/js/main.d4ca551a.chunk.js HTTP/1.1[0m" 304 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 22:06:15] "[36mGET /translate.png HTTP/1.1[0m" 304 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 22:06:15] "[36mGET /static/media/roboto-latin-500.f5b74d7f.woff2 HTTP/1.1[0m" 304 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 22:06:15] "[36mGET /static/media/roboto-latin-400.176f8f5b.woff2 HTTP/1.1[0m" 304 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 22:06:15] "[36mGET /static/media/roboto-latin-400italic.d022bc70.woff2 HTTP/1.1[0m" 304 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 22:06:15] "[36mGET /static/media/roboto-latin-700.c18ee39f.woff2 HTTP/1.1[0m" 304 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 22:06:15] "GET /translate.png HTTP/1.1" 200 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 22:06:15] "[36mGET /manifest.json HTTP/1.1[0m" 304 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 22:06:15] "[36mGET /logo192.png HTTP/1.1[0m" 304 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 22:06:57] "POST /translate HTTP/1.1" 200 -
INFO:root:Reloading encoder from model_1.pth ...
INFO:root:Reloading decoders from model_1.pth ...
DEBUG:root:Encoder: TransformerModel(
  (position_embeddings): Embedding(1024, 1024)
  (lang_embeddings): Embedding(3, 1024)
  (embeddings): Embedding(63961, 1024, padding_idx=2)
  (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  (attentions): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (layer_norm1): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (ffns): ModuleList(
    (0): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (1): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (2): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (3): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (4): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (5): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
  )
  (layer_norm2): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (pred_layer): PredLayer(
    (proj): Linear(in_features=1024, out_features=63961, bias=True)
  )
)
DEBUG:root:Decoder: [TransformerModel(
  (position_embeddings): Embedding(1024, 1024)
  (lang_embeddings): Embedding(3, 1024)
  (embeddings): Embedding(63961, 1024, padding_idx=2)
  (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  (attentions): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (layer_norm1): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (ffns): ModuleList(
    (0): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (1): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (2): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (3): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (4): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (5): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
  )
  (layer_norm2): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (layer_norm15): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (encoder_attn): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (pred_layer): PredLayer(
    (proj): Linear(in_features=1024, out_features=63961, bias=True)
  )
)]
INFO:root:Number of parameters (encoder): 142191065
INFO:root:Number of parameters (decoders): 167393753
INFO:root:Number of decoders: 1
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 22:07:20] "POST /translate HTTP/1.1" 200 -
INFO:root:Reloading encoder from model_1.pth ...
INFO:root:Reloading decoders from model_1.pth ...
DEBUG:root:Encoder: TransformerModel(
  (position_embeddings): Embedding(1024, 1024)
  (lang_embeddings): Embedding(3, 1024)
  (embeddings): Embedding(63961, 1024, padding_idx=2)
  (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  (attentions): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (layer_norm1): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (ffns): ModuleList(
    (0): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (1): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (2): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (3): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (4): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (5): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
  )
  (layer_norm2): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (pred_layer): PredLayer(
    (proj): Linear(in_features=1024, out_features=63961, bias=True)
  )
)
DEBUG:root:Decoder: [TransformerModel(
  (position_embeddings): Embedding(1024, 1024)
  (lang_embeddings): Embedding(3, 1024)
  (embeddings): Embedding(63961, 1024, padding_idx=2)
  (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  (attentions): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (layer_norm1): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (ffns): ModuleList(
    (0): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (1): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (2): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (3): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (4): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
    (5): TransformerFFN(
      (lin1): Linear(in_features=1024, out_features=4096, bias=True)
      (lin2): Linear(in_features=4096, out_features=1024, bias=True)
    )
  )
  (layer_norm2): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (layer_norm15): ModuleList(
    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
  )
  (encoder_attn): ModuleList(
    (0): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (1): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (2): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (3): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (4): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (5): MultiHeadAttention(
      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)
      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)
    )
  )
  (pred_layer): PredLayer(
    (proj): Linear(in_features=1024, out_features=63961, bias=True)
  )
)]
INFO:root:Number of parameters (encoder): 142191065
INFO:root:Number of parameters (decoders): 167393753
INFO:root:Number of decoders: 1
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 22:08:09] "POST /translate HTTP/1.1" 200 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 22:08:17] "POST /translate HTTP/1.1" 200 -
INFO:werkzeug:172.17.0.1 - - [01/Jul/2021 22:08:22] "POST /translate HTTP/1.1" 200 -
INFO:werkzeug: * Detected change in '/app/server/outputProcessing/outputFiles/outputFile.py', reloading
INFO:werkzeug: * Restarting with stat
WARNING:werkzeug: * Debugger is active!
INFO:werkzeug: * Debugger PIN: 279-760-745
